{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "julian-military",
   "metadata": {},
   "source": [
    "https://dacon.io/competitions/official/235738/codeshare/2902?page=1&dtype=recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "architectural-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import random\n",
    "# import glob\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "import scipy as sc\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from matplotlib import font_manager, rc, cm\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "%matplotlib inline\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "# from sktime.utils.plotting import plot_series\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import catboost\n",
    "from catboost import CatBoostRegressor\n",
    "from catboost import Pool\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 2\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# 음성 데이터 분석용 라이브러리\n",
    "import librosa\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dressed-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'D:\\Study\\Competition_English_Sound_Classification\\Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "indie-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "africa_train_paths = glob(data_path + r'\\train\\africa\\*.wav')\n",
    "australia_train_paths = glob(data_path + r'\\train\\australia\\*.wav')\n",
    "canada_train_paths = glob(data_path + r'\\train\\canada\\*.wav')\n",
    "england_train_paths = glob(data_path + r'\\train\\england\\*.wav')\n",
    "hongkong_train_paths = glob(data_path + r'\\train\\hongkong\\*.wav')\n",
    "us_train_paths = glob(data_path + r'\\train\\us\\*.wav')\n",
    "\n",
    "path_list = [africa_train_paths,\n",
    "             australia_train_paths,\n",
    "             canada_train_paths,\n",
    "             england_train_paths,\n",
    "             hongkong_train_paths,\n",
    "             us_train_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "blond-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paths = glob(data_path + r'\\test\\*.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "colonial-sucking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(paths):\n",
    "    result = []\n",
    "    \n",
    "    for path in tqdm(paths):\n",
    "        # sr(sampling rate) - 1초당 16000개의 데이터를 샘플링 한다는 의미\n",
    "        data, sr = librosa.load(path, sr = 16000)\n",
    "        result.append(data)\n",
    "        \n",
    "    result = np.array(result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_features(data, sr = 16000, n_fft = 2048, win_length = 200, hop_length = 160, n_mels = 64):\n",
    "    mel = []\n",
    "    for i in tqdm(data):\n",
    "        mel_ = librosa.feature.melspectrogram(i,\n",
    "                                              sr = sr,\n",
    "                                              n_fft = n_fft,\n",
    "                                              win_length = win_length,\n",
    "                                              hop_length = hop_length,\n",
    "                                              n_mels = n_mels)\n",
    "        mel.append(mel_)\n",
    "    mel = np.array(mel)\n",
    "    mel = librosa.power_to_db(mel, ref = np.max)\n",
    "    \n",
    "    return mel\n",
    "\n",
    "def set_length(data, d_mini):\n",
    "    result = []\n",
    "    for value in tqdm(data):\n",
    "        # 입력한 값(d_mini)을 기준으로 음성 데이터를 자른다.\n",
    "        # 입력한 값보다 더 짧은 데이터의 경우 남는 부분에 0을 채워넣어서 길이를 맞춘다.\n",
    "        value = value[:d_mini]\n",
    "        if len(value) < d_mini:\n",
    "            value = np.append(value, [0] * (d_mini - len(value)))\n",
    "            \n",
    "        result.append(np.array(value, dtype = np.float32))\n",
    "    \n",
    "    result = np.array(result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-potato",
   "metadata": {},
   "source": [
    "### 작업의 편의성을 위해 데이터를 numpy 파일로 변환하여 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "personal-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(os.path.join(data_path, 'npy_data')):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(os.path.join(data_path, 'npy_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "stupid-tyler",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [07:53<00:00,  5.28it/s]\n",
      "100%|██████████| 1000/1000 [03:10<00:00,  5.24it/s]\n",
      "100%|██████████| 1000/1000 [03:00<00:00,  5.53it/s]\n",
      "100%|██████████| 10000/10000 [32:43<00:00,  5.09it/s] \n",
      "100%|██████████| 1020/1020 [03:36<00:00,  4.72it/s]\n",
      "100%|██████████| 10000/10000 [34:17<00:00,  4.86it/s] \n"
     ]
    }
   ],
   "source": [
    "africa_train_data = load_data(africa_train_paths)\n",
    "australia_train_data = load_data(australia_train_paths)\n",
    "canada_train_data = load_data(canada_train_paths)\n",
    "england_train_data = load_data(england_train_paths)\n",
    "hongkong_train_data = load_data(hongkong_train_paths)\n",
    "us_train_data = load_data(us_train_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "controlled-skirt",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6100/6100 [18:55<00:00,  5.37it/s] \n"
     ]
    }
   ],
   "source": [
    "test_data = load_data(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "collected-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(os.path.join(data_path, r'npy_data/africa.npy'), africa_train_data)\n",
    "# np.save(os.path.join(data_path, r'npy_data/australia.npy'), australia_train_data)\n",
    "# np.save(os.path.join(data_path, r'npy_data/canada.npy'), canada_train_data)\n",
    "# np.save(os.path.join(data_path, r'npy_data/england.npy'), england_train_data)\n",
    "# np.save(os.path.join(data_path, r'npy_data/hongkong.npy'), hongkong_train_data)\n",
    "# np.save(os.path.join(data_path, r'npy_data/us.npy'), us_train_data)\n",
    "\n",
    "# np.save(os.path.join(data_path, r'npy_data/test.npy'), test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "material-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# africa_train_data = np.load(os.path.join(data_path, 'npy_data/africa.npy'), allow_pickle = True)\n",
    "# australia_train_data = np.load(os.path.join(data_path, 'npy_data/australia.npy'), allow_pickle = True)\n",
    "# canada_train_data = np.load(os.path.join(data_path, 'npy_data/canada.npy'), allow_pickle = True)\n",
    "# england_train_data = np.load(os.path.join(data_path, 'npy_data/england.npy'), allow_pickle = True)\n",
    "# hongkong_train_data = np.load(os.path.join(data_path, 'npy_data/hongkong.npy'), allow_pickle = True)\n",
    "# us_train_data = np.load(os.path.join(data_path, 'npy_data/us.npy'), allow_pickle = True)\n",
    "\n",
    "train_data_list = [africa_train_data, australia_train_data, canada_train_data, england_train_data, hongkong_train_data, us_train_data]\n",
    "\n",
    "test_data = np.load(os.path.join(data_path, 'npy_data/test.npy'), allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "super-thunder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500,)\n",
      "(1000,)\n",
      "(1000,)\n",
      "(10000,)\n",
      "(1020,)\n",
      "(10000,)\n",
      "\n",
      "(6100,)\n"
     ]
    }
   ],
   "source": [
    "for t_data in train_data_list:\n",
    "    print(t_data.shape)\n",
    "\n",
    "print()\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-april",
   "metadata": {},
   "source": [
    "### Data Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "stuck-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(data_path, r'npy_data/X_train_200.npy'), X_train_200)\n",
    "np.save(os.path.join(data_path, r'npy_data/X_train_400.npy'), X_train_400)\n",
    "np.save(os.path.join(data_path, r'npy_data/X_train_800.npy'), X_train_800)\n",
    "np.save(os.path.join(data_path, r'npy_data/X_train_1000.npy'), X_train_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fabulous-obligation",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 15.2 GiB for an array with shape (25520, 64, 626, 4) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-88e91a73e06a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# X_train_1000 = X_train_1000.reshape(X_train_1000.shape[0], X_train_1000.shape[1], X_train_1000.shape[2], 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m X_train_multi = np.concatenate([X_train_200,\n\u001b[0m\u001b[0;32m     16\u001b[0m                                 \u001b[0mX_train_400\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                                 \u001b[0mX_train_800\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 15.2 GiB for an array with shape (25520, 64, 626, 4) and data type float32"
     ]
    }
   ],
   "source": [
    "# X_train = np.concatenate(train_data_list, axis = 0)\n",
    "# X_train = set_length(X_train, 100000)\n",
    "\n",
    "# X_train_200 = get_features(data = X_train, win_length = 200)\n",
    "# X_train_400 = get_features(data = X_train, win_length = 400)\n",
    "# X_train_800 = get_features(data = X_train, win_length = 800)\n",
    "# X_train_1000 = get_features(data = X_train, win_length = 1000)\n",
    "\n",
    "# \n",
    "# X_train_200 = X_train_200.reshape(X_train_200.shape[0], X_train_200.shape[1], X_train_200.shape[2], 1)\n",
    "# X_train_400 = X_train_400.reshape(X_train_400.shape[0], X_train_400.shape[1], X_train_400.shape[2], 1)\n",
    "# X_train_800 = X_train_800.reshape(X_train_800.shape[0], X_train_800.shape[1], X_train_800.shape[2], 1)\n",
    "# X_train_1000 = X_train_1000.reshape(X_train_1000.shape[0], X_train_1000.shape[1], X_train_1000.shape[2], 1)\n",
    "\n",
    "X_train_multi = np.concatenate([X_train_200,\n",
    "                                X_train_400,\n",
    "                                X_train_800,\n",
    "                                X_train_1000], -1)\n",
    "np.save(os.path.join(data_path, r'npy_data/X_train_multi.npy'), X_train_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-sessions",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load(os.path.join(data_path, r'npy_data/test.npy'), allow_pickle = True)\n",
    "X_test = set_length(X_test, 100000)\n",
    "\n",
    "X_test_200 = get_feature(data = X_test, win_length = 200)\n",
    "X_test_400 = get_feature(data = X_test, win_length = 400)\n",
    "X_test_800 = get_feature(data = X_test, win_length = 800)\n",
    "X_test_1000 = get_feature(data = X_test, win_length = 1000)\n",
    "\n",
    "X_test_200 = X_test_200.reshape(X_test_200.shape[0], X_test_200.shape[1], X_test_200.shape[2], 1)\n",
    "X_test_400 = X_test_400.reshape(X_test_400.shape[0], X_test_400.shape[1], X_test_400.shape[2], 1)\n",
    "X_test_800 = X_test_800.reshape(X_test_800.shape[0], X_test_800.shape[1], X_test_800.shape[2], 1)\n",
    "X_test_1000 = X_test_1000.reshape(X_test_1000.shape[0], X_test_1000.shape[1], X_test_1000.shape[2], 1)\n",
    "X_test_multi = np.concatenate([X_test_200,\n",
    "                               X_test_400,\n",
    "                               X_test_800,\n",
    "                               X_test_1000], axis = 1)\n",
    "np.save(os.path.join(data_path, r'npy_data/X_test_multi.npy'), X_test_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-direction",
   "metadata": {},
   "source": [
    "# Network & DataLoader\n",
    "\n",
    "- 네트워크는 Conv2d, BatchNorm2d, ReLU 를 합친 block으로 주로 구성되어있고, output 전 dropout, AvgPool, AdaptiveAvgPool을 적용했습니다.\n",
    "- 데이터를 배치단위로 불러올 때 train 데이터의 분포로 min-max scaling 해주었습니다.\n",
    "- 1/2 확률로 데이터를 rolling 하여 augmentation 효과를 노려봤지만 큰 도움이 되지는 않았던거 같아서 이것은 적용하지 않았습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cardiovascular-picking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(input_, units = 32, dropout_rate = 0.5):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-tiffany",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-pixel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-clinic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-violence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-french",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_bn_relu(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(conv_bn_relu, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        self.BN = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.BN(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, N):\n",
    "        super(Network, self).__init__()\n",
    "        self.N = N\n",
    "        self.AveragePooling = nn.AvgPool2d(2)\n",
    "        self.MaxPooling = nn.MaxPool2d(2)\n",
    "        \n",
    "        \n",
    "        self.input_conv = conv_bn_relu(in_channels=4, out_channels=self.N, kernel_size=3)\n",
    "        \n",
    "        self.block1 = nn.Sequential(\n",
    "            conv_bn_relu(in_channels=self.N*1, out_channels=self.N*2, kernel_size=3),\n",
    "            conv_bn_relu(in_channels=self.N*2, out_channels=self.N*4, kernel_size=3),\n",
    "            conv_bn_relu(in_channels=self.N*4, out_channels=self.N*2, kernel_size=3),\n",
    "            conv_bn_relu(in_channels=self.N*2, out_channels=self.N*1, kernel_size=3),\n",
    "        )\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            conv_bn_relu(in_channels=self.N*1, out_channels=self.N*2, kernel_size=3),\n",
    "            conv_bn_relu(in_channels=self.N*2, out_channels=self.N*4, kernel_size=3),\n",
    "            conv_bn_relu(in_channels=self.N*4, out_channels=self.N*2, kernel_size=3),\n",
    "            conv_bn_relu(in_channels=self.N*2, out_channels=self.N*1, kernel_size=3),\n",
    "        )\n",
    "        \n",
    "        self.block3 = nn.Sequential(\n",
    "            conv_bn_relu(in_channels=self.N*1, out_channels=self.N*2, kernel_size=3),\n",
    "            conv_bn_relu(in_channels=self.N*2, out_channels=self.N*4, kernel_size=3),\n",
    "            conv_bn_relu(in_channels=self.N*4, out_channels=self.N*2, kernel_size=3),\n",
    "            conv_bn_relu(in_channels=self.N*2, out_channels=self.N*1, kernel_size=3),\n",
    "        )\n",
    "        \n",
    "        self.block4 = nn.Sequential(\n",
    "            conv_bn_relu(in_channels=self.N*1, out_channels=self.N*2, kernel_size=3),\n",
    "            conv_bn_relu(in_channels=self.N*2, out_channels=self.N*4, kernel_size=3),\n",
    "            conv_bn_relu(in_channels=self.N*4, out_channels=self.N*2, kernel_size=3),\n",
    "            conv_bn_relu(in_channels=self.N*2, out_channels=self.N*1, kernel_size=3),\n",
    "        )\n",
    "        \n",
    "        self.pool_block = nn.Sequential(\n",
    "            conv_bn_relu(in_channels=self.N*1, out_channels=self.N*2, kernel_size=3),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.AvgPool2d(2),\n",
    "            conv_bn_relu(in_channels=self.N*2, out_channels=self.N*2, kernel_size=3),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.AvgPool2d(2),\n",
    "            conv_bn_relu(in_channels=self.N*2, out_channels=self.N*2, kernel_size=3),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=self.N*2, out_features=6),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, out=''):\n",
    "        x = self.input_conv(x)\n",
    "        x = self.MaxPooling(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.pool_block(x)\n",
    "        x=self.output(x)\n",
    "        \n",
    "        if out=='sigmoid':\n",
    "            x = F.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class VoiceDatasetSimple(Dataset):\n",
    "        def __init__(self, X, y, transform, inference=False, roll=False):\n",
    "            self.X = X\n",
    "            self.y = y\n",
    "            self.transform = transform\n",
    "            self.inference = inference\n",
    "            self.roll = roll\n",
    "        def __len__(self):\n",
    "            return len(self.X)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            X = self.X[idx]\n",
    "            X = (X-train_x_min)/(train_x_max-train_x_min)\n",
    "            \n",
    "            if self.inference:\n",
    "                X = self.transform(X)\n",
    "                return X\n",
    "            else:\n",
    "                if (self.roll==True) and (random.randint(0, 1)==1):\n",
    "                    X = np.roll(X,random.randint(-200, 200), axis=1)\n",
    "                    \n",
    "                X = self.transform(X)    \n",
    "                y = self.y[idx]\n",
    "                \n",
    "                onehot = np.zeros(6)\n",
    "                onehot[y] = 1.\n",
    "                y = onehot\n",
    "                return X, y\n",
    "\n",
    "def model_save(model, path):\n",
    "    torch.save({\n",
    "        'model': model.state_dict(),\n",
    "    }, path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
