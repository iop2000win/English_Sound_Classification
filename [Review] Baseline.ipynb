{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f11048c6",
   "metadata": {},
   "source": [
    "https://dacon.io/competitions/official/235738/codeshare/2688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15fb8bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil # ?\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "# 음성 처리를 위한 패키지\n",
    "import librosa\n",
    "\n",
    "# 오류 메세지 처리를 위한 패키지\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3ce4088",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('./Data/sample_submission.csv')\n",
    "\n",
    "africa_train_paths = glob('./Data/train/africa/*.wav')\n",
    "australia_train_paths = glob('./Data/train/australia/*.wav')\n",
    "canada_train_paths = glob('./Data/train/canada/*.wav')\n",
    "england_train_paths = glob('./Data/train/england/*.wav')\n",
    "hongkong_train_paths = glob('./Data/train/hongkong/*.wav')\n",
    "us_train_paths = glob('./Data/train/us/*.wav')\n",
    "\n",
    "path_list = [africa_train_paths,\n",
    "             australia_train_paths,\n",
    "             canada_train_paths,\n",
    "             england_train_paths,\n",
    "             hongkong_train_paths,\n",
    "             us_train_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab2397f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paths = glob('./Data/test/*.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13a7adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(paths):\n",
    "    result = []\n",
    "    for path in tqdm(paths):\n",
    "        data, sr = librosa.load(path, sr = 16000)\n",
    "        \n",
    "        result.append(data)\n",
    "    \n",
    "    result = np.array(result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "406e9020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2500/2500 [23:22<00:00,  1.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# 매번 음성 파일을 읽어와 처리하려면 시간이 많이 소요되므로, 한 번 읽어온 후,\n",
    "# npy 파일로 변형하여 데이터들을 저장한다.\n",
    "if os.path.isdir('./Data/npy_data'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir('./Data/npy_data')\n",
    "\n",
    "# africa_train_data = load_data(africa_train_paths)\n",
    "# np.save('./Data/npy_data/africa_npy', africa_train_data)\n",
    "\n",
    "# australia_train_data = load_data(australia_train_paths)\n",
    "# np.save('./Data/npy_data/australia_npy', australia_train_data)\n",
    "\n",
    "# canada_train_data = load_data(canada_train_paths)\n",
    "# np.save('./Data/npy_data/canada_npy', canada_train_data)\n",
    "\n",
    "# england_train_data = load_data(england_train_paths)\n",
    "# np.save('./Data/npy_data/enlgand_npy', england_train_data)\n",
    "\n",
    "# hongkong_train_data = load_data(hongkong_train_paths)\n",
    "# np.save('./Data/npy_data/hongkong_npy', hongkong_train_data)\n",
    "\n",
    "# us_train_data = load_data(us_train_paths)\n",
    "# np.save('./Data/npy_data/us_npy', us_train_data)\n",
    "\n",
    "# test_data = load_data(test_paths)\n",
    "# np.save('./Data/npy_data/test_npy', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e788447",
   "metadata": {},
   "outputs": [],
   "source": [
    "africa_train_data = np.load('./Data/npy_data/africa_npy.npy', allow_pickle = True)\n",
    "australia_train_data = np.load(\"./Data/npy_data/australia_npy.npy\", allow_pickle = True)\n",
    "canada_train_data = np.load(\"./Data/npy_data/canada_npy.npy\", allow_pickle = True)\n",
    "england_train_data = np.load(\"./Data/npy_data/england_npy.npy\", allow_pickle = True)\n",
    "hongkong_train_data = np.load(\"./Data/npy_data/hongkong_npy.npy\", allow_pickle = True)\n",
    "us_train_data = np.load(\"./Data/npy_data/us_npy.npy\", allow_pickle = True)\n",
    "\n",
    "test_data = np.load(\"./Data/npy_data/test_npy.npy\", allow_pickle = True)\n",
    "\n",
    "train_data_list = [africa_train_data, australia_train_data, canada_train_data, england_train_data, hongkong_train_data, us_train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e16dc3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "africa: (2500,)\n",
      "australia: (1000,)\n",
      "canada: (1000,)\n",
      "england: (10000,)\n",
      "hongkong: (1020,)\n",
      "us: (10000,)\n",
      "test: (6100,)\n"
     ]
    }
   ],
   "source": [
    "print(f'africa: {africa_train_data.shape}')\n",
    "print(f'australia: {australia_train_data.shape}')\n",
    "print(f'canada: {canada_train_data.shape}')\n",
    "print(f'england: {england_train_data.shape}')\n",
    "print(f'hongkong: {hongkong_train_data.shape}')\n",
    "print(f'us: {us_train_data.shape}')\n",
    "\n",
    "print(f'test: {test_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "883c49fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113664,)\n",
      "(81024,)\n",
      "(84864,)\n",
      "(87168,)\n",
      "(104064,)\n",
      "(123264,)\n"
     ]
    }
   ],
   "source": [
    "for t in train_data_list:\n",
    "    print(t[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20bf79b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각각의 음성 파일의 길이가 모두 다르다 >>> 최소값을 기준으로 맞춰서 데이터 사용\n",
    "def get_mini(data):\n",
    "    mini = 9999999\n",
    "    for i in data:\n",
    "        if len(i) < mini:\n",
    "            mini = len(i)\n",
    "            \n",
    "    return mini\n",
    "\n",
    "# 위 함수를 통한 최솟값으로 파이들의 길이 맞추기\n",
    "def set_length(data, d_mini):\n",
    "    result = []\n",
    "    for i in data:\n",
    "        result.append(i[:d_mini])\n",
    "    \n",
    "    result = np.array(result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# feature 생성\n",
    "def get_feature(data, sr = 16000, n_fft = 256, win_length = 200, hop_length = 160, n_mels = 64):\n",
    "    mel = []\n",
    "    for i in data:\n",
    "        mel_ = librosa.feature.melspectrogram(i,\n",
    "                                              sr = sr,\n",
    "                                              n_fft = n_fft,\n",
    "                                              win_length = win_length,\n",
    "                                              hop_length = hop_length,\n",
    "                                              n_mels = n_mels)\n",
    "        mel.append(mel_)\n",
    "        \n",
    "    mel = np.array(mel)\n",
    "    mel = librosa.power_to_db(mel, ref = np.max)\n",
    "    \n",
    "    mel_mean = mel.mean()\n",
    "    mel_std = mel.std()\n",
    "    mel = (mel - mel_mean) / mel_std\n",
    "    \n",
    "    return mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5b3643f",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.05 GiB for an array with shape (25520, 64, 501) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-61a16b9dc486>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmini\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-35e1dee7f888>\u001b[0m in \u001b[0;36mget_feature\u001b[1;34m(data, sr, n_fft, win_length, hop_length, n_mels)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mmel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmel_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mmel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mmel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower_to_db\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.05 GiB for an array with shape (25520, 64, 501) and data type float32"
     ]
    }
   ],
   "source": [
    "X_train = np.concatenate(train_data_list, axis = 0)\n",
    "X_test = np.array(test_data)\n",
    "\n",
    "train_mini = get_mini(X_train)\n",
    "test_mini = get_mini(X_test)\n",
    "mini = np.min([train_mini, test_mini])\n",
    "\n",
    "X_train = set_length(X_train, mini)\n",
    "X_test = set_length(X_test, mini)\n",
    "\n",
    "X_train = get_feature(data = X_train)\n",
    "X_test = get_feature(data = X_test)\n",
    "\n",
    "X_train = X_train.reshape(-1, X_train.shape[1], X_train.shape[2], 1)\n",
    "X_test = X_test.reshape(-1, X_test.shape[1], X_test.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935e8b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.concatenate((np.zeros(len(africa_train_data), dtype = np.int),\n",
    "                          np.ones(len(australia_train_data), dtype = np.int),\n",
    "                          np.ones(len(canada_train_data), dtype = np.int),\n",
    "                          np.ones(len(england_train_data), dtype = np.int),\n",
    "                          np.ones(len(hongkong_train_data), dtype = np.int),\n",
    "                          np.ones(len(us_train_data), dtype = np.int)), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda3a858",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X_train: {X_train.shape}')\n",
    "print(f'y_train: {y_train.shape}')\n",
    "print(f'X_test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4f3ae5",
   "metadata": {},
   "source": [
    "# Modeling - 2D CNN 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c86c84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Convolution2D, BatchNormalization, Flatten,\n",
    "                                      Dropout, Dense, AveragePooling2D, Add)\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afca4da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(input_, units = 32, dropout_rate = 0.5):\n",
    "    x = Convolution2D(units, 3, padding = 'same', activation = 'relu')(input_)\n",
    "    x = BatchNormalization()(x)\n",
    "    x_res = x # ???\n",
    "    \n",
    "    x = Convolution2D(units, 3, padding = 'same', activation = 'relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Convolution2D(units, 3, padding = 'same', activation = 'relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Add()([x, x_res])\n",
    "    x = AveragePooling2D()(x)\n",
    "    x = Dropout(rate = dropout_rate)(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def second_block(input_, units = 64, dropout_rate = 0.5):\n",
    "    x = Convolution2D(units, 1, padding = 'same', activation = 'relu')(input_)\n",
    "    x = Convolution2D(units, 3, padding = 'same', activation = 'relu')(x)\n",
    "    x = Convolution2D(units * 4, 1, padding = 'same', activation = 'relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x_res = x\n",
    "    \n",
    "    x = Convolution2D(units, 1, padding = 'same', activation = 'relu')(x)\n",
    "    x = Convolution2D(units, 3, padding = 'same', activation = 'relu')(x)\n",
    "    x = Convolution2D(units * 4, 1, padding = 'same', activation = 'relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Convolution2D(units, 1, padding = 'same', activation = 'relu')(x)\n",
    "    x = Convolution2D(units, 3, padding = 'same', activation = 'relu')(x)\n",
    "    x = Convolution2D(units * 4, 1, padding = 'same', activation = 'relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Add()([x, x_res])\n",
    "    x = AveragePooling2D()(x)\n",
    "    x = Dropout(rate = dropout_rate)()\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dddcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fn():\n",
    "    dropout_rate = 0.3\n",
    "    \n",
    "    in_ = Input(shape = (train_x.shape[1:]))\n",
    "    \n",
    "    block_01 = block(in_, units = 32, dropout_rate = dropout_rate)\n",
    "    block_02 = block(block_01, units = 64, dropout_rate = dropout_rate)\n",
    "    block_03 = block(block_02, units = 128, dropout_rate = dropout_rate)\n",
    "    \n",
    "    block_04 = second_block(block_03, units = 64, dropout_rate = dropout_rate)\n",
    "    block_05 = second_block(block_04, units = 128, dropout_rate = dropout_rate)\n",
    "    \n",
    "    x = Flatten()(block_05)\n",
    "    \n",
    "    x = Dense(units = 128, activation = 'relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x_res = x\n",
    "    x = Dropout(rate = dropout_rate)(x)\n",
    "    \n",
    "    x = Dense(units = 128, activation = 'relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x_res, x])\n",
    "    x = Dropout(rate = dropout_rate)(x)\n",
    "    \n",
    "    model_out = Dense(units = 6, activation = 'softmax')(x)\n",
    "    model = Model(in_, model_out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d87143",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 10)\n",
    "\n",
    "pred = []\n",
    "pred_ = []\n",
    "\n",
    "for train_idx, val_idx in split.split(train_x, train_y):\n",
    "    x_train, y_train = train_x[train_idx], train_y[train_idx]\n",
    "    x_val, y_val = train_x[val_idx], train_y[val_idx]\n",
    "    \n",
    "    model = build_fn()\n",
    "    model.compile(optimizer = keras.optimizers.Adam(0.002),\n",
    "                  loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics = ['acc'])\n",
    "    \n",
    "    history = model.fit(x = x_train,\n",
    "                        y = y_train,\n",
    "                        validation_data = (x_val, y_val),\n",
    "                        epochs = 8)\n",
    "    print('*'*100)\n",
    "    pred.append(model.predict(test_x))\n",
    "    pred_.append(np.argmax(model.predict(test_x), axis = 1))\n",
    "    print('*'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c74ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_type(data):\n",
    "    return np.int(data)\n",
    "\n",
    "result = pd.concat([test_, pd.DataFrame(np.mean(pred, axis = 0))], axis = 1).iloc[:, 1:]\n",
    "result['id'] = result['id'].apply(lambda x: cov_type(x))\n",
    "\n",
    "result = pd.merge(sample_submission['id'], result)\n",
    "result.columns = sample_submission.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
