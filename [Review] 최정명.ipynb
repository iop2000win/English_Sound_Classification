{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accredited-dinner",
   "metadata": {},
   "source": [
    "https://dacon.io/competitions/official/235738/codeshare/2902?page=1&dtype=recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "previous-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import random\n",
    "# import glob\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "import scipy as sc\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from matplotlib import font_manager, rc, cm\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "%matplotlib inline\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "# from sktime.utils.plotting import plot_series\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import catboost\n",
    "from catboost import CatBoostRegressor\n",
    "from catboost import Pool\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 2\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# 음성 데이터 분석용 라이브러리\n",
    "import librosa\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "incoming-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'D:\\Study\\Competition_English_Sound_Classification\\Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "sixth-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "africa_train_paths = glob(data_path + r'\\train\\africa\\*.wav')\n",
    "australia_train_paths = glob(data_path + r'\\train\\australia\\*.wav')\n",
    "canada_train_paths = glob(data_path + r'\\train\\canada\\*.wav')\n",
    "england_train_paths = glob(data_path + r'\\train\\england\\*.wav')\n",
    "hongkong_train_paths = glob(data_path + r'\\train\\hongkong\\*.wav')\n",
    "us_train_paths = glob(data_path + r'\\train\\us\\*.wav')\n",
    "\n",
    "path_list = [africa_train_paths,\n",
    "             australia_train_paths,\n",
    "             canada_train_paths,\n",
    "             england_train_paths,\n",
    "             hongkong_train_paths,\n",
    "             us_train_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "reserved-russia",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paths = glob(data_path + r'\\test\\*.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "distinct-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(paths):\n",
    "    result = []\n",
    "    \n",
    "    for path in tqdm(paths):\n",
    "        # sr(sampling rate) - 1초당 16000개의 데이터를 샘플링 한다는 의미\n",
    "        data, sr = librosa.load(path, sr = 16000)\n",
    "        result.append(data)\n",
    "        \n",
    "    result = np.array(result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_features(data, sr = 16000, n_fft = 2048, win_length = 200, hop_length = 160, n_mels = 64):\n",
    "    mel = []\n",
    "    for i in tqdm(data):\n",
    "        mel_ = librosa.feature.melspectrogram(i,\n",
    "                                              sr = sr,\n",
    "                                              n_fft = n_fft,\n",
    "                                              win_length = win_length,\n",
    "                                              hop_length = hop_length,\n",
    "                                              n_mels = n_mels)\n",
    "        mel.append(mel_)\n",
    "    mel = np.array(mel)\n",
    "    mel = librosa.power_to_db(mel, ref = np.max)\n",
    "    \n",
    "    return mel\n",
    "\n",
    "def set_length(data, d_mini):\n",
    "    result = []\n",
    "    for value in tqdm(data):\n",
    "        value = value[:d_mini]\n",
    "        if len(value) < d_mini:\n",
    "            value = np.append(value, [0] * (d_mini - len(value)))\n",
    "            \n",
    "        result.append(value)\n",
    "    \n",
    "    result = np.array(resutl)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-publisher",
   "metadata": {},
   "source": [
    "### 작업의 편의성을 위해 데이터를 numpy 파일로 변환하여 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "anticipated-handle",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(os.path.join(data_path, 'npy_data')):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(os.path.join(data_path, 'npy_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "second-liver",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [07:53<00:00,  5.28it/s]\n",
      "100%|██████████| 1000/1000 [03:10<00:00,  5.24it/s]\n",
      "100%|██████████| 1000/1000 [03:00<00:00,  5.53it/s]\n",
      "100%|██████████| 10000/10000 [32:43<00:00,  5.09it/s] \n",
      "100%|██████████| 1020/1020 [03:36<00:00,  4.72it/s]\n",
      "100%|██████████| 10000/10000 [34:17<00:00,  4.86it/s] \n"
     ]
    }
   ],
   "source": [
    "africa_train_data = load_data(africa_train_paths)\n",
    "# np.save(os.path.join(data_path, 'africa.npy', africa_train_data)\n",
    "\n",
    "australia_train_data = load_data(australia_train_paths)\n",
    "# np.save(\"./dataset/australia-sorted.npy\", australia_train_data)\n",
    "\n",
    "canada_train_data = load_data(canada_train_paths)\n",
    "# np.save(\"./dataset/canada-sorted.npy\", canada_train_data)\n",
    "\n",
    "england_train_data = load_data(england_train_paths)\n",
    "# np.save(\"./dataset/england-sorted.npy\", england_train_data)\n",
    "\n",
    "hongkong_train_data = load_data(hongkong_train_paths)\n",
    "# np.save(\"./dataset/hongkong-sorted.npy\", hongkong_train_data)\n",
    "\n",
    "us_train_data = load_data(us_train_paths)\n",
    "# np.save(\"./dataset/us-sorted.npy\", us_train_data)\n",
    "# train_data_list = [africa_train_data, australia_train_data, canada_train_data, england_train_data, hongkong_train_data, us_train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "developmental-slovak",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6100/6100 [18:55<00:00,  5.37it/s] \n"
     ]
    }
   ],
   "source": [
    "test_data = load_data(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "rubber-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(os.path.join(data_path, r'npy_data/africa.npy'), africa_train_data)\n",
    "# np.save(os.path.join(data_path, r'npy_data/australia.npy'), australia_train_data)\n",
    "# np.save(os.path.join(data_path, r'npy_data/canada.npy'), canada_train_data)\n",
    "# np.save(os.path.join(data_path, r'npy_data/england.npy'), england_train_data)\n",
    "# np.save(os.path.join(data_path, r'npy_data/hongkong.npy'), hongkong_train_data)\n",
    "# np.save(os.path.join(data_path, r'npy_data/us.npy'), us_train_data)\n",
    "\n",
    "np.save(os.path.join(data_path, r'npy_data/test.npy'), test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load(os.path.join(data_path, '/npy_data/africa.npy'), allow_pickle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-incentive",
   "metadata": {},
   "source": [
    "### Data Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate(train_data_list, axis = 0)\n",
    "X_train = set_length(X_train, 100000)\n",
    "\n",
    "X_train_200 = get_feature(data = X_train, win_length = 200)\n",
    "X_train_400 = get_feature(data = X_train, win_length = 400)\n",
    "X_train_800 = get_feature(data = X_train, win_length = 800)\n",
    "X_train_1000 = get_feature(data = X_train, win_length = 1000)\n",
    "\n",
    "# \n",
    "X_train_200 = X_train_200.reshape(X_train_200.shape[0], X_train_200.shape[1], X_train_200.shape[2], 1)\n",
    "X_train_400 = X_train_400.reshape(X_train_400.shape[0], X_train_400.shape[1], X_train_400.shape[2], 1)\n",
    "X_train_800 = X_train_800.reshape(X_train_800.shape[0], X_train_800.shape[1], X_train_800.shape[2], 1)\n",
    "X_train_1000 = X_train_1000.reshape(X_train_1000.shape[0], X_train_1000.shape[1], X_train_1000.shape[2], 1)\n",
    "\n",
    "X_train_multi = np.concatenate([X_train_200,\n",
    "                                X_train_400,\n",
    "                                X_train_800,\n",
    "                                X_train_1000], -1)\n",
    "np.save(os.path.join(data_path, r'npy_data/X_train_multi.npy'), X_train_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load(os.path.join(data_path, r'npy_data/test.npy'), allow_pickle = True)\n",
    "X_test = set_length(X_test, 100000)\n",
    "\n",
    "X_test_200 = get_feature(data = X_test, win_length = 200)\n",
    "X_test_400 = get_feature(data = X_test, win_length = 400)\n",
    "X_test_800 = get_feature(data = X_test, win_length = 800)\n",
    "X_test_1000 = get_feature(data = X_test, win_length = 1000)\n",
    "\n",
    "X_test_200 = X_test_200.reshape(X_test_200.shape[0], X_test_200.shape[1], X_test_200.shape[2], 1)\n",
    "X_test_400 = X_test_400.reshape(X_test_400.shape[0], X_test_400.shape[1], X_test_400.shape[2], 1)\n",
    "X_test_800 = X_test_800.reshape(X_test_800.shape[0], X_test_800.shape[1], X_test_800.shape[2], 1)\n",
    "X_test_1000 = X_test_1000.reshape(X_test_1000.shape[0], X_test_1000.shape[1], X_test_1000.shape[2], 1)\n",
    "X_test_multi = np.concatenate([X_test_200,\n",
    "                               X_test_400,\n",
    "                               X_test_800,\n",
    "                               X_test_1000], axis = 1)\n",
    "np.save(os.path.join(data_path, r'npy_data/X_test_multi.npy'), X_test_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-renewal",
   "metadata": {},
   "source": [
    "# Network & DataLoader\n",
    "\n",
    "- 네트워크는 Conv2d, BatchNorm2d, ReLU 를 합친 block으로 주로 구성되어있고, output 전 dropout, AvgPool, AdaptiveAvgPool을 적용했습니다.\n",
    "- 데이터를 배치단위로 불러올 때 train 데이터의 분포로 min-max scaling 해주었습니다.\n",
    "- 1/2 확률로 데이터를 rolling 하여 augmentation 효과를 노려봤지만 큰 도움이 되지는 않았던거 같아서 이것은 적용하지 않았습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
